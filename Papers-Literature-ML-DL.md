
reading research paper :
https://www.youtube.com/watch?v=SHTOI0KtZnU--siraj reading reseach paper
https://medium.com/ai-saturdays/how-to-read-academic-papers-without-freaking-out-3f7ef43a070f

{https://www.reddit.com/r/MachineLearning/
https://www.reddit.com/r/MachineLearning/
https://research.google.com}

[https://github.com/pursh2002/Papers-Literature-ML-DL
[https://peerj.com/articles/453.pdf
[http://www.marekrei.com/blog/paper-summaries/?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3BVGtsKX8YShGi8i5T5gy%2BjA%3D%3D

These are the 5 papers that we will be covering in the AMA on 5am-10am Friday 26th of January 2018 Coordinated Universal Time (UTC).

[https://nurture.ai/p/41166f98-40d8-4af8-bc89-3b2146c9a456---A Multi-Agent Reinforcement Learning Model of Common-Pool Resource Appropriation
[https://nurture.ai/p/b1007b2c-3be4-46e6-9d46-094461f51614---Data Augmentation by Pairing Samples for Image Classification
[https://nurture.ai/p/eb6fd99b-a0cc-4aec-ab42-1a19465198f4---Generating Adversarial Examples with Adversarial Networks
[https://nurture.ai/p/ff91053e-7115-4dce-b346-d39e74583703---Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks
[https://nurture.ai/p/a02d8113-36a4-4783-954f-a48d37098b20eCommerce---GAN: A Generative Adversarial Network for E-commerce15

[https://medium.com/applied-data-science/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188
[https://arxiv.org/pdf/1502.05698.pdf

[https://medium.com/applied-data-science/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188

[Nurture.AI Research Fellows are hosting yet another round of AMA for this week's top AI research papers! https://lnkd.in/fisPtT6

Here are the papers they are covering this week, with annotations in comments by our research fellows:
PointCNN: https://lnkd.in/fzN2Mdn
Letâ€™s Dance: Learning From Online Dance Videos: https://lnkd.in/fTyskYH
PRNN: Recurrent Neural Network with Persistent Memory: https://lnkd.in/fCuCnq5
HappyDB: A Corpus of 100,000 Crowdsourced Happy Moments: https://lnkd.in/fjsHZsJ
Personalizing Dialogue Agents: I have a dog, do you have pets too?: https://lnkd.in/fmyPhpK


[https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73

https://goo.gl/nJr5KH deep learnig

## These are the 5 papers that we will be covering in the AMA on 5am-10am Friday 26th of January 2018 Coordinated Universal Time (UTC).

https://nurture.ai/p/41166f98-40d8-4af8-bc89-3b2146c9a456-- A Multi-Agent Reinforcement Learning Model of Common-Pool Resource Appropriation23
https://nurture.ai/p/b1007b2c-3be4-46e6-9d46-094461f51614-- Data Augmentation by Pairing Samples for Image Classification12
https://nurture.ai/p/eb6fd99b-a0cc-4aec-ab42-1a19465198f4-- Generating Adversarial Examples with Adversarial Networks9
https://nurture.ai/p/ff91053e-7115-4dce-b346-d39e74583703--Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks6
https://nurture.ai/p/a02d8113-36a4-4783-954f-a48d37098b20-- eCommerceGAN: A Generative Adversarial Network for E-commerce21


https://github.com/pursh2002/DL-2

https://www.arxiv-vanity.com/papers/1802.07740/?utm_content=buffera6f07&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer

https://arxiv.org/pdf/1611.01491.pdf
https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf
https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf


ML 
Statistics, Probability and Calculus.
http://www2.math.uu.se/~thulin/mm/breiman.pdf -- good start to understand the views of statistical and computer science research community about Statistical Learning

http://vita.had.co.nz/papers/tidy-data.pdf -- data preparation and preprocessing

https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/

https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf

Data Representation: Where you transform data to another space (usually vector space).
https://arxiv.org/pdf/1301.3781.pdf --- words in vector space
https://cs.stanford.edu/~quocle/paragraph_vector.pdf -- A document to vector

Learning:
http://ruder.io/optimizing-gradient-descent/
http://www.maths.ed.ac.uk/~prichtar/papers/Papamakarios.pdf
https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/

Evaluation: Empirical evaluation of a model is very important, to understand the model and infer from it.

https://www.analyticsvidhya.com/blog/2016/02/7-important-model-evaluation-error-metrics/
http://pages.cs.wisc.edu/~dpage/cs760/evaluating.pdf
http://www.damienfrancois.be/blog/files/modelperfcheatsheet.pdf%20(http://www.damienfrancois.be/blog/files/modelperfcheatsheet.pdf

Paper:
https://lnkd.in/dPC2h-R
Code:
https://lnkd.in/dJzCHMi


https://arxiv.org/abs/1603.08155
http://research.nvidia.com/publication/2017-10_Progressive-Growing-of
https://news.ycombinator.com/item?id=15572790
https://github.com/Avhirup/Progressive-Growing-Of-GANs-Pytorch-

https://arxiv.org/abs/1506.01186

http://forums.fast.ai/t/wiki-lesson-2/9399

https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html

https://www.jeremyjordan.me/neural-networks-training/

GAN : https://lnkd.in/ddCmS9j?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3BO4efC7IAT7irll63pQZKoA%3D%3D


Are CNNs better than RNNs (including LSTM and GRU) for sequence modeling? 

This is the strong claim of an important new paper from Carnegie Mellon and Intel: An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling, by Shaojie Bai, J. Zico Kolter, Vladlen Koltun.

Authors carried out an extensive comparison of simple CNNs vs LSTM and GRU on a large number of tasks. According to the authors, their general CNNs:

"convincingly outperform baseline recurrent architectures across a broad range of sequence modeling tasks."

"exhibit substantially longer memory, and are thus more suitable for domains where a long history is required."

"Until recently, before the introduction of architectural elements such as dilated convolutions and residual connections, convolutional architectures were indeed weaker. Our results indicate that with these elements, a simple convolutional architecture is more effective across diverse sequence modeling tasks than recurrent architectures such as LSTMs.

"We conclude that convolutional networks should be regarded as a natural starting point and a powerful toolkit for sequence modeling."

Arxiv: https://lnkd.in/f2kr6dE
Github: https://lnkd.in/fY5TTDd
